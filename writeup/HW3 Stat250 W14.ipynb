{
 "metadata": {
  "name": "",
  "signature": "sha256:b1a7012049875eccfe2784aec51dd7f9e4e3a3a2b80c3a6aa9511f8158644875"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Author: Karen Ng <karenyng@ucdavis.edu> \n",
      "\n",
      "<a href=\"https://github.com/karenyng/HW3_Stat250_W14\" target=\"_blank\">Github repository for this HW</a>\n",
      "\n",
      "Written with an IPython notebook v. 2.0.0-dev"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autosave 60\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Background of <a href =\"http://eeyore.ucdavis.edu/stat250/Homeworks/hw3.html\" target=\"_blank\">assignment</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We explore <a href=\"https://web.archive.org/web/20130609062934/http://stackoverflow.com/help/closed-questions\" target=\"_blank\">Stack Overflow</a> questions that should be closed.\n",
      "This classification problem comes from a \n",
      "<a href=\"https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow\"  target=\"_blank\">Kaggle competition</a>.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Personal goals:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* learn how to use existing statistical packages / machine learning functions \n",
      "* learn the underlying considerations when preparing the data for those functions \n",
      "* learn the workflow of data analysis \n",
      "* not get distracted "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Content "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This approximately corresponds to my approximate workflow but I will not have time to do everything."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(click link below to jump to particular session of this IPython notebook.\n",
      "This may not work with the pdf version - in that case, open up the bookmarks of the pdf to jump to particular section of this document)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Get data to remote machine](#Getting-the-data-using-the-cookie-and-curl)\n",
      "    * small training set \n",
      "    * bigger 6GB full data set \n",
      "* [Data exploration](#Data-exploration) \n",
      "    * examine nature / relationship between different variables\n",
      "    * examine how actual content of questions helps ....\n",
      "    * convert markdown to pure text \n",
      "    * do NLP for the content of the questions \n",
      "    * create suitable new variables\n",
      "    * verify data integrity after transformation / reduction \n",
      "    * make simplifying assumptions \n",
      "    * MAYBE remove low signal-to-noise data points / outliers\n",
      "    * massage data to look like the form that the stat / ML functions want\n",
      "* [Background of Stat/ML techniques: Random Forest](#Random-Forest)\n",
      "* [Analysis](#Analysis)\n",
      "    * get a 2-case classification done (open / close) just using the user info to get intuition \n",
      "    * get a 2-case then 4-case classification done with the different tags \n",
      "    * get a 2-case then 4-case classification done with the different tags \n",
      "* try to use more than one technique to verify the results are not insane\n",
      "\n",
      "* [Performance](#Performance)\n",
      "\n",
      "    * do cross validation \n",
      "        * look at point percent misclassification \n",
      "        * look at the oob score estimate\n",
      "    * look at variable importance - rethink strategy\n",
      "    * maybe redo RF again after tuning the input parameters based on performance\n",
      "* [Limitations from software side](#Limitations-from-the-software-side)\n",
      "* [Code](#Code) \n",
      "    * Dependencies \n",
      "    * Actual code"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data sets used"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting the data using the cookie and curl"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Google Chrome also shows content of cookie in the developer tool but the attributes seem to be chopped up and cannot be easily cut and paste to be used with curl. \n",
      "\n",
      "So I had to use my second favorite browser firefox to extract the cookie."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1) train.csv - training data "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    curl -L --cookie \"MYFIREFOXCOOKIE\" https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/download/train.7z > train.7z\n",
      "    \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2) entire data dump "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    curl -L --cookie \"MYFIREFOXCOOKIE\" https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/download/2012-07%20Stack%20Overflow.7z > 2012-07-Stack-Overflow.7z"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How to get all the csv files from the Kaggle data website:\n",
      "* there must be something in the XML package (or other packages) that Duncan wrote that can get all the links, by searching for the attribute \"a href\"\n",
      "And we can use RCurl within R to download the csv files \n",
      "* download html code of the website, use sed / awk to filter out the links which are after \"a href\" and inside quotes, then use Curl in the shell\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Choice of programming language / packages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's hard to decide because R seems like a really good choice but I am also very slow when using R. I want to use python since I want to focus on learning about the statistics / ML method than learning about programming / a programming language."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<a href=\"http://nbviewer.ipython.org/github/karenyng/HW3_Stat250_W14/blob/master/writeup/Data%20exploration%20.ipynb\" target=\"_blank\">Data exploration</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Click the link above to see another IPython notebook with other detailed code, plots and discussion.\n",
      "Here I will just put the most important findings."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data properties"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training data - train.csv"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> size: ~3.5 GB \n",
      "\n",
      "> no. of observations in training data = 3.37 M\n",
      "\n",
      "> no. of open questions =  3.30 M, 97.92% of total observations \n",
      "\n",
      "> no. of closed questions = 70.1 K, 2.08% of total observations \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the number of fields (variables) is ~10 << 70.1K ,\n",
      "we should feel free to create more variables to summarize the context of the questions. \n",
      "\n",
      "Note the percentage break down of train.csv is not what's claimed in the SO website. A further breakdown of the reasons reveal:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table>\n",
      "<tr><td>Reason for closing question</td><td>% out of all closed questions</td>\n",
      "<tr>\n",
      "<td> too localized </td><td>8.78% </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td> not constructive </td><td>22.33% </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td> off topic </td><td>24.99% </td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td> not a real question </td><td>43.90% </td>\n",
      "</tr>\n",
      "</table>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* not sure if it's ok to use these numbers as prior.\n",
      "* if there is only ~9% of the questions being in the \"too localized\" category, it might not be easy to train it "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data reduction and preprocessing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Duncan gave a <a href=\"https://github.com/karenyng/stat250/blob/master/notes/15.md\" target=\"_blank\">good lecture</a> the other day about how to create variables out of the content of the questions, i.e. preprocessing data. That got me wondering if all of the given variables are actually relevant..... \n",
      "\n",
      "And I came across <a href=\"http://www.kdnuggets.com/2014/03/data-mining-do-and-dont.html\" target=\"_blank\">this article</a> about data-mining do-and-don't which emphasizes data ETL, preparation etc.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stripping markdown from text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I actually installed the javascript engine node.js on my remote machines for use. They are slightly awkward and I found that Python has its own modules for converting markdown to plain text. \n",
      "\n",
      "The use is a little tricky since the modules seem to like utf-8 as the encoding but the default python's string encoding is ascii.\n",
      "\n",
      "* convert markdown to html using the markdown module\n",
      "* use default functions from nltk to strip the html code \n",
      "* do stemming and other precprocessing steps using nltk"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Given variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perhaps not all of them are useful"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* PostCreationDate\n",
      "* OwnerUserId\n",
      "* OwnerCreationDate\n",
      "* ReputationAtPostCreation\n",
      "* OwnerUndeletedAnswerCountAtPostTime\n",
      "* Title\n",
      "* BodyMarkdown\n",
      "* Tag1\n",
      "* Tag2\n",
      "* Tag3\n",
      "* Tag4\n",
      "* Tag5\n",
      "\n",
      "It seems likely that Tag 1-5 should be related to the actual content of the of the question and should be analyzed together. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Create new variables "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To know what new variables would be helpful involves guesswork and is not easy to do well."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Random Forest (RF) from scikit learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The documentation from Rpart is very nice and I am very tempted but I do not want to switch language in the middle of the analysis! Will use it next time!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "characteristics "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Snag: \n",
      "When we add new data / variables, we have to rebuild our trees...\n",
      "\n",
      "* can use the out of bag data points for doing cross validation, it is easy to show that $N_{oob} \\approx N_{total} e^{-1}$ data points will not be used to build the trees\n",
      "\n",
      "\n",
      "* ranked as one of the best algorithms in Kaggle competitions etc.\n",
      "\n",
      "* Summary from Elements of statistical learning regarding the use of random forest:\n",
      "\n",
      "    * select $m$ out of $p$ variables at each split, \n",
      "    * it recommends using $m = \\sqrt{p} $ and the minimum node size is $1$ for classification \n",
      "    * but we need to tune $m$ after first trying the values of $m$ out \n",
      "    * when $m$ is small and the fraction of relevant variables is small, then RF may perform poorly\n",
      "    * when the # of relevant variables is sufficient, RF will perform more robustly - so the creation of relevant variables is important......."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Usage"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a bunch of parameters that I should set / think about: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* max_features = this is tree specific but since we know the features we want to test so we set this \n",
      "    * 2 for the testing purpose to see how well it does for just classifying open and closed cases\n",
      "    * 5 for the real deal\n",
      "    \n",
      "* criterion = \"gini\" cofficient is the default criteria for splitting and I think it's fine.\n",
      "    \n",
      "* max_depth = \n",
      "\n",
      "* bootstrap = Yes! Not sure why there 's a choice.....\n",
      "\n",
      "* oob_score = True ! please do the entire homework for me.\n",
      "\n",
      "* n_jobs = 4 \n",
      "    * very nice that the developers had parallelization in mind when designing the package\n",
      "\n",
      "* verbose = True \n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Comparison to other methods "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "choice of parameters "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Results "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What are good predictors? "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Importance of variables given by the trees"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The format requires by "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross validation "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Performance"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Misclassification rate (or percent correct classification PCC) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I subset the data for cross validation "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Discussion about the package used"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I read the documentation of Rpart and it seems to be:\n",
      "* a very good tutorial about the underlying statistics \n",
      "* "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Code"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dependencies "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* pandas "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Machine specification "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Actual code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}